{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset0 Consolas;}{\f2\fnil\fcharset1 Cambria Math;}{\f3\fnil\fcharset161 Calibri;}{\f4\fnil Consolas;}}
{\colortbl ;\red224\green226\blue228;}
{\*\generator Riched20 10.0.14393}{\*\mmathPr\mmathFont2\mwrapIndent1440 }\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang9 Smart Cab Driving Agent\par

\pard\sa200\sl276\slmult1\fs40 Introduction\fs22\par
The Objective of this project is to implement the Q-learning bellmann algorithm to model and train a car(Agent) to navigate through various traffic scenarios to reach the final destination.we model the states and actions from different peices of information we collect at any given point of the program execution.All of the interesting steps have been implementd in the 'ageny.py' file of the project, it contains:\par
Global Variables:\par
\f1\fs20\lang1033 q        : A dictionary holds the Q-values for all states exlored so far(Changing)\par
actions  : holds the set of actions permissible for each state(Constant)\par
trial    : holds the current trail number \par
\f0\fs22 Class and Methods:\par
\f1\fs20 LearningAgent(Agent):\par
     def _init_      : Initializes the Environment and required variables\par
     def reset       : Resets/changes any variable after each trail\par
     def update      : Chooses the best action in a state and updates the q-value\par
     def sigmoid     : Computes the logistic function value of the input\par
     def update_q    : Implements the Q-learning algorithm to update q-value\par
     def build_state : Builds a state from given inputs\par
     def get_max_q   : Returns the maximum q-value for a given state\par
     def max_random  : Returns a random Maximum from in case of multiple max's\par
\par
\f0\fs40 Implement a Basic Driving Agent\par
\fs22 We implement a basic driving agent by choosing a random action at each traffic intersection regardless of any other state information.The agent, as expected, shows no pattern of learning as it progresses throught the 100 Trials run in this mode.As it executes a random action each time, it bounces around the environment and sometimes reaches the destination , most times it doesn't.\par
\fs40\par
Inform the Driving Agent\par
\fs22 Next, we model the State space.Each state if a tuple made of:\par
oncoming    : Is there oncoming traffic? if so, which way is it going?\par
left          : Is there Traffic on the left(perpendicular road)? if so, which way is it going?\par
light         : Green or Red?\par
deadline      : (Time left to reach destination)/5\par
waypoint     : Next  waypoint (left or right or Foreward)\par
\par
we need to know Oncoming and Left traffic to Learn to avoid accidents in similar traffic situations.\par
we need to know Traffic Light information to avoid accidents and negative penalty in similar situations.\par
we need to know next Way point to avoid negative reward for wrong actions.\par
we need to Know deadline to take a bad action/negative reward action such an action rewards us in the Long run, as in the case where time is running out so we could run a red light when no cars are present to reach the destination in Time.In our simulation the average deadline is less than 50 Units, which increases the state space drastically, so we divide it into intervals of '5' Units each(< 10 intervals on average)\par
The input 'Right' is not taken into consideration as a conclusion from the following argument:\par
Considering the other cars at any intersection obey Traffic Rules, the only way a car on the right would matter is\par
\tab Light is Red and we try to go Forward or Left, if we go right we would not conflict with \tab  \tab the car on the right.\par
we could learn not to violate the Red light from other input variables and actions(for the Most part) .Hence, 'Right' could be considered redundant information. \par
With these as our parameters, our State space contains on average < 3*3*2*10*3 = 540 states\par
Note that most of these states won't be reached as our agent learns and optimizes its decisions.For 100 Trials, i found that the number of states explored was around 100 - 130 range.Notice that the Deadline input contributes to 90% of the states signifying the importance of reaching the destination in time.\par
This input(deadline)is interesting as generally, as the agent learns, it gets closer to the destination as the deadline approaches zero which, in a sense is global information about its position relative to the destination while all other inputs provide no information other than immidiate surroundings.Using this input helps us make use of the potential future reward(by tweaking the Discount factor)information in the algorithm. Having more states also gives us the freedom to explore many other unusual/exceptions states and improve accuracy depending on the amount of training time we wish to spend.\par
\fs40 Implement a Q-Learning Driving Agent\fs22\par
Now that we have our states and actions modeled, it is time to implement the Q- learning algorithm : \par
Q[s,a] \f2\u8592?\f0 Q[s,a] + \f3\lang1032\'e1(\f0\lang1033 (\f3\lang1032 r+ \'e3maxa' Q[s',a']\f0\lang1033 )\f3\lang1032  - Q[s,a])\par
\f0\lang1033 Where : \tab Q[s,a] is the q-value for each state-action pair\par
\tab\tab '\f3\lang1032\'e1\f0\lang1033 '  is the Learning factor - range [0,1]\par
\tab\tab '\f3\lang1032\'e3\f0\lang1033 ' is the Discount factor - range [0,1]\par
we add any unvisited states to the 'q' dictionary and initialize all four                                                       q-values(None,Left,right,forward) to Zero as this means neither of the actions hold positive or negative rewards, they are neutral.\par
we then initialize '\f3\lang1032\'e1\f0\lang1033 ' to 1\par
we initialize '\f3\lang1032\'e3\f0\lang1033 ' to 0\par
As we run the simulation, there is a clear learning pattern as it progresses through each trail and after about 50 trails, the agent reached the destination more often than not as is expected.\par
As the agent progresses though each trial, it accumulates information about the rules of the game, which actions result in negative reward & which actions result in positive reward in the form of q-values for each Q[s,a] pair.\par
the Algorithm in each state essentially \par
- checks the max q-value for the state\par
- performs that action to reach  "  s'  "\par
- checks the max q-value for the new state and discounts it by '\f3\lang1032\'e3\f0\lang1033 '  : \f3\lang1032\'e3maxa' Q[s',a']\par
\f0\lang1033 - adds the intrinsic reward for that action taken and subtracts the q-value of the previous     state : (\f3\lang1032 r+ \'e3maxa' Q[s',a']\f0\lang1033 )\f3\lang1032  - Q[s,a]\f0\lang1033 ) , this gives us the "net gain" in reward for the action we just took.\par
- Discounts this gain further by '\f3\lang1032\'e1\f0\lang1033 ', as how much we want to learn from this single action instance : \f3\lang1032\'e1(\f0\lang1033 (\f3\lang1032 r+ \'e3maxa' Q[s',a']\f0\lang1033 )\f3\lang1032  - Q[s,a])\par
\f0\lang1033 - adds this Discounted gain(+ve or -ve) to the q-value of the previous state  ' s ' \par
These steps essentially accumulates and store the knowledge from previous trials and hence our agent is able to perform significantly better than when it took Random actions.\par
After Implementing the q-learning parameters and running the simulation, there is a significant improvement in the success rate of the agent.In the intial trials < 10 , there is little to no improvement and the cars do get stuck in loops that could represent local minimum.But as the q-values update, these loops are broken and the agent follows the next_waypoint with more accuracy and gets closer to the destination every few steps (as seen in the Pygame smulation) and eventually after about 30 trails, it reaches the destination most of the time.\par
One more point to note is, After running the simulation for 100 trials, the agent explores, on average only 80 -100 states ,far less than the total possible states(540) and less than when we introduce the exploration factor(100-130) as we see in the next section.\par
\fs40 Improve the Q-Learning Driving Agent\par
\fs22\par
\fs28 Exploration rate \fs22\par
Next, we try to improve our learning agent by tweaking some of the parameters.First, we Tune the exploration rate epsilon - '\f3\lang1032\u1013?\f0\lang1033 '.This parameter essentially controlls how we explore the state space:\par
At each state, should we blindly choose the action with max q-value? what if the difference in q-values is very small or even '0' and other actions need some some exploration to find their value more accurately?.\par
To address these questions, at each state we choose an action Randomly with some probability even if there is a max q-valued action.But we should want to do this with less and less probability as we run through more iterations of our model as we would like to increase Trust in our gathered q-values.\par
 In the code, this is implemented by comparing a random float(0 -1) to the output of a scaled sigmoid function with 100 number of trails being the max input.\par
\par
\fs28 Learning rate\fs22\par
The learning rate '\f3\lang1032\'e1\f0\lang1033 ' is set to 1 initially as we would like to learn everything from a Q[s,a] due to the nature of this simulation.As the rewards, rules and actions are NOT probabilistic. \par
The exact same Learned situation at a future point of time would give us the same reward(exept for the final destination) and hence lowering this would likely not give us any benifit.\par
'\f3\lang1032\'e1\f0\lang1033 ' rates of 0 to 1 have been tried with 50 trails with 0.1 increments and the general trend is the accuracy increases as '\f3\lang1032\'e1\f0\lang1033 ' increased.\par
\fs28\par
Discount Factor\fs22\par
The Discount factor '\f3\lang1032\'e3\f0\lang1033 ' is set to 0 initially through the following argument:\par
- The discount factor esentially weights the long term value of an Q[s,a] pair.For our simulation, the start and Destination change for each Trial and so do the traffic at a given  - waypoint,Trial.So the 'Future Value' of a particular Q[s,a] is dependent, mostly on the closeness of the state to the Destination and we not have this information explicitly.Hence this information is not very reliable and hence be discounted severly.\par
But, we have modeled our state to include Classes of size '5' each of the deadline.And a future value of a Q[s,a] could be relavent in the following scenario:\par
- we are close to converging at the optimal policy and the Deadline class is '1' i.e Deadline <= 5, and a Q[s,a] has high q-value even though it violates some rules, because in previous trials, it reached the destination in time by incurring some negative reward at the very end(deadline class 1 or 2),which means it is near its destination(as it is a close to optimal agent by now) to gain a reward of '12' at the destination.\par
In this case, we do get some 'future value' information from the q-value.\par
values of 0 - 1 have been tried with 0.1 increments and there is noticable decline in converging time as '\f3\lang1032\'e3\f0\lang1033 ' increases from  about 0.4  - 1.\par
\fs40 Exploring the Parameter Space\par
\fs22 Below is a Snapshot of my Trail runs : \par
\par
Learning\tab discount\tab\tab success(x/50)(with epsilon)\tab success(no eplilon)\par
1\tab\tab\tab 0\tab\tab\tab 25\tab\tab\tab\tab 46\tab\par
 0.8\tab\tab\tab 0\tab\tab\tab 25\tab\tab\tab\tab 46\par
0.6\tab\tab\tab 0\tab\tab\tab 22\tab\tab\tab\tab 45\par
0.4\tab\tab\tab 0\tab\tab\tab 20\tab\tab\tab\tab 46\tab\par
0.2\tab\tab\tab 0\tab\tab\tab 21\tab\tab\tab\tab 45\par
0.1\tab\tab\tab 0\tab\tab\tab 21\tab\tab\tab\tab 45\par
1\tab\tab\tab 0.2\tab\tab\tab 25\tab\tab\tab\tab 46\tab\par
1\tab\tab\tab 0.4\tab\tab\tab 24\tab\tab\tab\tab 45\tab\par
1\tab\tab\tab 0.6\tab\tab\tab 19\tab\tab\tab\tab 44\par
1\tab\tab\tab 0.8\tab\tab\tab 20\tab\tab\tab\tab 45\par
1\tab\tab\tab 1\tab\tab\tab 19\tab\tab\tab\tab 46\par
\par
The optimal policy for this simulation would be one where the Agent reaches the Destination Every Time Within the given deadline and does not Incurr any negative rewards.\par

\pard After tuning the Parameters and training it for 100 Trials every training cycle, the agent reaches its destination its Destination in time on average 9/10 times on each fresh Training cycle and on each Trail run after each Training cycle, the agent incurrs < =1 negative penalties on average, depending on the number of waypoints in the path.I would say this agent is close to its optimal policy.\par
\par
From the Parameter Space table(50 trails on each observation) above we can gather the following general conclusions:\par
\par
- As a general trend, success rate decreases (converges slowly) as learning rate decreases from 1 to 0.\par

\pard\sa200\sl276\slmult1 -As the Discount factor increases from 0 to 1, there is no general linear trend in success, values of 0,0.2 seem to be slightly better than the others.Although Statistically significant differences were not found for  0 - 0.3 , i settled at a gamma value of 0.2 in hopes of succeding in capturing some future value in some exeptional cases(all else being equal).\par

\pard - The most important observation would be that the absense of epsilon for our simulation tends to increase the convergence rate Drastically.Hence, the best option would be to exclude epsilon altogether, atleast for out particular simulation.\par
\par
A plausible argument for this anomoly could be that our simulation is simple(few rules) and the agent needs only a few states(examples) to learn about the rules of the game and exploring random states to "Learn" more is a redundant choice and so it converges sooner without this random exploration.In a More complex game though, epsilon would be a good decision to argue for.\par
\par

\pard\sa200\sl276\slmult1 After this analysis, The final parameters i settled on are : \par
\tab Learning rate '\f3\lang1032\'e1\f0\lang1033 '       - 1, from the range [0,1]\par
\tab Discount factor '\f3\lang1032\'e3\f0\lang1033 '   - 0.2, from the range [0,1]\par
\tab Exploration rate '\f3\lang1032\u1013?\f0\lang1033 '   - sigmoid(nth_trial)\par
\tab n_trials (training)      - 100 \par
\cf1\f4\fs20\lang9\par
}
 